from typing import Any

import torch
from torch import nn
from torchvision import transforms


class Autoencoder(nn.Module):
    """Convolutional autoencoder for generating object embeddings.

    Args:
    ----
        embedding_size (int): The size of the latent embedding.

    Attributes:
    ----------
        encoder (nn.Sequential): The encoder part of the autoencoder.
        decoder (nn.Sequential): The decoder part of the autoencoder.

    """

    def __init__(self, embedding_size: int = 128) -> None:  # noqa: D107
        super(Autoencoder, self).__init__()  # noqa: UP008
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Flatten(),
            nn.Linear(64 * 16 * 16, embedding_size),
        )
        self.decoder = nn.Sequential(
            nn.Linear(embedding_size, 64 * 16 * 16),
            nn.Unflatten(1, (64, 16, 16)),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid(),
        )

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """Forward pass through the autoencoder.

        Args:
        ----
            x (torch.Tensor): Input image tensor.

        Returns:
        -------
            Tuple[torch.Tensor, torch.Tensor]:
            The encoded embedding and the reconstructed image.

        """
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded


def get_object_embeddings(  # noqa: D417
    frames: list[torch.Tensor], detected_objects: list[dict[str, Any]]
) -> list[dict[str, Any]]:
    """Generate object embeddings using the trained autoencoder.

    Args:
    ----
        frames (List[torch.Tensor]): List of video frames.
        detected_objects (List[Dict[str, Any]]):
        List of detected objects, where each object is
        represented as a dictionary with keys:
            'vidId', 'frameNum', 'detectedObjId', 'detectedObjClass',
            'confidence', 'bbox'.

    Returns:
    -------
        List[Dict[str, Any]]: List of dictionaries, where each dictionary contains the
        object's information and its embedding.

    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = Autoencoder().to(device)
    # Load the trained model weights

    embeddings = []
    for frame, objects in zip(frames, detected_objects, strict=False):
        for obj in objects:
            x1, y1, x2, y2 = obj["bbox"]
            crop = frame[:, y1:y2, x1:x2]
            crop = transforms.Resize((64, 64))(crop)
            crop = transforms.ToTensor()(crop).unsqueeze(0).to(device)
            _, encoded = model(crop)
            embeddings.append(
                {
                    "vidId": obj["vidId"],
                    "frameNum": obj["frameNum"],
                    "detectedObjId": obj["detectedObjId"],
                    "detectedObjClass": obj["detectedObjClass"],
                    "confidence": obj["confidence"],
                    "bbox": obj["bbox"],
                    "embedding": encoded.squeeze().cpu().numpy(),
                }
            )
    return embeddings
